# 摘要

主要的序列转导模型是基于复杂的递归或卷积神经网络，其中包括一个编码器和一个解码器。表现最好的模型也通过注意力机制连接编码器和解码器。

我们提出了一个新的简单的网络架构，`Transformer`，完全基于注意力机制，完全免除递归和卷积。

通过将Transformer成功应用于大量和有限训练数据的英语选区解析，表明Transformer可以很好地推广到其他任务

# 1	介绍

循环模型通常沿着输入和输出序列的符号位置进行计算。在计算时间中将位置与步骤对齐，它们生成一个隐藏层$h_t$序列，作为前面的隐藏层$h_{t-1}$和位置$t$的输入的函数。这种固有的顺序性质排除了训练示例中的并行化，这在较长的序列长度中变得至关重要，因为内存限制了跨示例的批处理。

注意力机制允许在不考虑输入或输出序列中的距离的情况下建模依赖关系。然而，除了少数情况外，这种注意力机制都是与循环网络结合使用的。

# 2	背景

关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离而增长.

在Transformer中，这被减少为一个常数级的操作，尽管代价是由于平均注意力加权位置而降低了有效分辨率，我们用Multi-Head Attention抵消了这一影响。

自注意力机制，是一种将单个序列的不同位置联系起来的注意力机制，以便计算序列的表示。

# 3	模型架构

编码器映射一个由符号表示的输入序列$(x_1,\dots,x_n)$到连续表示序列$z=(z_1,\dots,z_n)$。给定$z$，解码器生成一个符号输出序列$(y_1,\dots,y_n)$(一次生成一个元素)。在每一步中，模型都是自回归的，并在生成下一步时，将先前生成的符号作为额外的输入。

Transformer遵循这种总体架构，为编码器和解码器使用了自注意力堆栈和按点的全连接层，分别如图1的左右两部分所示。

<img src="D:%5CTypora%5Cuser-image%5C1677936891583.png" alt="1677936891583" style="zoom:67%;" />

## 3.1	编码器和解码器堆栈

**编码器：**编码器由具有$N=6$个相同层的堆栈组成。每层有两个子层。第一个是一个多头自注意力机制，第二个是一个简单的、按位置全连接的前馈网络。在`每两个子层`周围使用一个残余连接，然后进行层归一化。每个子层的输出是$LayerNorm(x+Sublayer(x))$，其中$Sublayer(x)$是子层本身实现的函数。为了方便这些残余连接，模型中的所有子层以及embedding层都会产生维度为$d_{model}=512$的输出。

**解码器：**解码器也由N = 6个相同层的堆栈组成。除了每个编码器层中的两个子层外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意力操作。与编码器类似，在`每个子层`周围使用残余连接，然后进行层归一化。我们还修改了解码器堆栈中的自注意力子层，以防止position关注后续position。这种掩蔽，结合输出embedding被一个position抵消的事实，确保对位置$i$的预测只能依赖于小于$i$位置的已知输出。

`图1的左半部分为编码层，右半部分为解码层`

## 3.2	Attention

注意力函数可以描述为将查询和一组键-值对映射到输出，其中查询、键、值和输出都是向量。输出是按值的加权和计算的，其中分配给每个值的权重是通过查询与相应键的兼容性函数计算的。

### 3.2.1	缩放的点积注意力

![1678014183771](D:%5CTypora%5Cuser-image%5C1678014183771.png)

