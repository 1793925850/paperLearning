# 摘要

主要的序列转导模型是基于复杂的递归或卷积神经网络，其中包括一个编码器和一个解码器。表现最好的模型也通过注意力机制连接编码器和解码器。

我们提出了一个新的简单的网络架构，`Transformer`，完全基于注意力机制，完全免除递归和卷积。

通过将Transformer成功应用于大量和有限训练数据的英语选区解析，表明Transformer可以很好地推广到其他任务

# 1	介绍

循环模型通常沿着输入和输出序列的符号位置进行计算。在计算时间中将位置与步骤对齐，它们生成一个隐藏层$h_t$序列，作为前面的隐藏层$h_{t-1}$和位置$t$的输入的函数。这种固有的顺序性质排除了训练示例中的并行化，这在较长的序列长度中变得至关重要，因为内存限制了跨示例的批处理。

注意力机制允许在不考虑输入或输出序列中的距离的情况下建模依赖关系。然而，除了少数情况外，这种注意力机制都是与循环网络结合使用的。

# 2	背景

关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离而增长.

在Transformer中，这被减少为一个常数级的操作，尽管代价是由于平均注意力加权位置而降低了有效分辨率，我们用Multi-Head Attention抵消了这一影响。

自注意力机制，是一种将单个序列的不同位置联系起来的注意力机制，以便计算序列的表示。

# 3	模型架构

编码器映射一个由符号表示的输入序列$(x_1,\dots,x_n)$到连续表示序列$z=(z_1,\dots,z_n)$。给定$z$，解码器生成一个符号输出序列$(y_1,\dots,y_n)$(一次生成一个元素)。在每一步中，模型都是自回归的，并在生成下一步时，将先前生成的符号作为额外的输入。

Transformer遵循这种总体架构，为编码器和解码器使用了自注意力堆栈和按点的全连接层，分别如图1的左右两部分所示。

<img src="D:%5CTypora%5Cuser-image%5C1677936891583.png" alt="1677936891583" style="zoom:67%;" />

## 3.1	编码器和解码器堆栈

**编码器：**编码器由具有$N=6$个相同层的堆栈组成。每层有两个子层。第一个是一个多头自注意力机制，第二个是一个简单的、按位置全连接的前馈网络。在`每两个子层`周围使用一个残余连接，然后进行层归一化。每个子层的输出是$LayerNorm(x+Sublayer(x))$，其中$Sublayer(x)$是子层本身实现的函数。为了方便这些残余连接，模型中的所有子层以及embedding层都会产生维度为$d_{model}=512$的输出。

**解码器：**解码器也由N = 6个相同层的堆栈组成。除了每个编码器层中的两个子层外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意力操作。与编码器类似，在`每个子层`周围使用残余连接，然后进行层归一化。我们还修改了解码器堆栈中的自注意力子层，以防止position关注后续position。这种掩蔽，结合输出embedding被一个position抵消的事实，确保对位置$i$的预测只能依赖于小于$i$位置的已知输出。

`图1的左半部分为编码层，右半部分为解码层`

## 3.2	Attention

注意力函数可以描述为将查询和一组键-值对映射到输出，其中查询、键、值和输出都是向量。输出是按值的加权和计算的，其中分配给每个值的权重是通过查询与相应键的兼容性函数计算的。

### 3.2.1	Scaled Dot-Product Attention

![1678014183771](D:%5CTypora%5Cuser-image%5C1678014183771.png)

输入由维度为$d_k$的查询和键以及维度为$d_v$的值组成。计算查询的所有键的点积，每个键除以$\sqrt{d_k}$，然后使用一个softmax函数来获得值的权重。

在实验中，同时计算一组查询上的注意力函数，将它们打包成一个矩阵$Q$。键和值对应的矩阵为$K$和$V$。计算输出矩阵为：
$$
\tag{1}
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$
最常用的两个注意函数是`加性注意`和`点积注意`(乘性注意)。

加性注意算法采用带有单一隐藏层的前馈网络计算兼容性函数。点积注意在实践中要更快、更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。

对于$d_k$值较大时，点积的大小会变大，将softmax函数推到具有极小梯度的区域，为了抵消这种影响，将点积乘以$\frac{1}{\sqrt{d_k}}$。

### 3.2.2	Multi-Head Attention

Multi-head attention使模型能够同时关注来自不同位置的不同表示子空间的信息。如果只有一个注意力头，平均会抑制这一点。
$$
MultiHead(Q,K,V)=Concat(head_1,\dots,head_h)W^O
\\
where \ head_i=Attention(QW^Q_i,KW^K_i, VW^V_i)
$$
其中，投影是参数矩阵$W^Q_i\in \R^{d_{model}\times d_k}$，$W^K_i\in \R^{d_{model}\times d_k}$，$W^V_i\in \R^{d_{model}\times d_v}$和$W^O_i\in \R^{hd_v\times d_{model}}$

使用h = 8个并行注意力层

$d_k=d_v=d_{model}/h=64$

### 3.2.3	Attention在模型中的应用

Transformer以三种不同的方式使用多头注意力：

- 在“编码器-解码器注意”层中，查询来自前一个解码器层，键和值来自编码器的输出。这使得解码器中的每个位置都可以覆盖输入序列中的所有位置。这模拟了序列到序列模型中的典型编码器-解码器注意机制
- 编码器包含自注意力层。在自注意力层中，所有的键、值和查询都来自同一个地方，即编码器中前一层的输出。编码器中的每个位置都可以关注到编码器前一层中的所有位置。
- 解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。需要防止信息在解码器中向左流动，以保持自回归特性。我们通过屏蔽掉(也就是Masked，设置为−∞)softmax输入中对应非法连接的所有值来实现缩放点积注意。



## 3.3	位置前馈网络

除了注意力子层，编码器和解码器中的每一层都包含一个全连接的前馈网络，该网络分别且相同地应用于每个位置。这包括两个线性转换，中间有一个ReLU激活。
$$
\tag{2}
FFN(x)=max(0,xW_1+b_1)W_2+b_2
$$
其中，线性变换在不同position上是相同的，但在不同层之间使用不同的参数。另一种描述方法是两个核大小为1的卷积。输入输出的维数为$d_{model}=512$，内层维数为$d_{ff}=2048$。

## 3.4	Embedding和Softmax

使用Embedding将输入token和输出token转换为维度为$d_{model}$的向量。使用线性变换和softmax函数将解码器的输出转换为下一个token的预测概率。在两个embedding层之间共享相同的权重矩阵和pre-softmax线性变换。在embedding层中，将这些权重乘以$\sqrt{d_{model}}$。

## 3.5	位置编码

必须增加一些有关token的绝对或相对位置信息到序列中。

因此，将“位置编码”添加到编码器和解码器堆栈底部的输入embedding中。位置编码与enbedding有相同的维数$d_{model}$，因此两者可以想加。

这项工作中，使用了不同频率的正弦和余弦函数：
$$
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})
\\
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})
$$
其中，$pos$是位置，$i$是维度。即位置编码的每个维度对应一个正弦波。波长形成从$2\pi$到$10000\cdot 2\pi$的几何级数。假设它可以让模型很容易地学会通过相对位置来参与，因为对于任何固定偏移量k，$PE_{pos+k}$可以表示为$PE_{pos}$的线性函数。

选择了正弦版本，因为它可以允许模型外推到比训练期间遇到的序列长度更长的序列。

# 4	为什么选择自注意力