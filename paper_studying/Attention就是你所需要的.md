# 摘要

主要的序列转导模型是基于复杂的递归或卷积神经网络，其中包括一个编码器和一个解码器。表现最好的模型也通过注意力机制连接编码器和解码器。

我们提出了一个新的简单的网络架构，`Transformer`，完全基于注意力机制，完全免除递归和卷积。

通过将Transformer成功应用于大量和有限训练数据的英语选区解析，表明Transformer可以很好地推广到其他任务

# 1、介绍

循环模型通常沿着输入和输出序列的符号位置进行计算。在计算时间中将位置与步骤对齐，它们生成一个隐藏层$h_t$序列，作为前面的隐藏层$h_{t-1}$和位置$t$的输入的函数。这种固有的顺序性质排除了训练示例中的并行化，这在较长的序列长度中变得至关重要，因为内存限制了跨示例的批处理。

注意力机制允许在不考虑输入或输出序列中的距离的情况下建模依赖关系。然而，除了少数情况外，这种注意力机制都是与循环网络结合使用的。

# 2、背景

关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离而增长.

在Transformer中，这被减少为一个常数级的操作，尽管代价是由于平均注意力加权位置而降低了有效分辨率，我们用Multi-Head Attention抵消了这一影响。

自注意力机制，是一种将单个序列的不同位置联系起来的注意力机制，以便计算序列的表示。

