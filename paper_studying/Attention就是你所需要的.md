# 摘要

主要的序列转导模型是基于复杂的递归或卷积神经网络，其中包括一个编码器和一个解码器。表现最好的模型也通过注意力机制连接编码器和解码器。

我们提出了一个新的简单的网络架构，`Transformer`，完全基于注意力机制，完全免除递归和卷积。

通过将Transformer成功应用于大量和有限训练数据的英语选区解析，表明Transformer可以很好地推广到其他任务

# 1	介绍

循环模型通常沿着输入和输出序列的符号位置进行计算。在计算时间中将位置与步骤对齐，它们生成一个隐藏层$h_t$序列，作为前面的隐藏层$h_{t-1}$和位置$t$的输入的函数。这种固有的顺序性质排除了训练示例中的并行化，这在较长的序列长度中变得至关重要，因为内存限制了跨示例的批处理。

注意力机制允许在不考虑输入或输出序列中的距离的情况下建模依赖关系。然而，除了少数情况外，这种注意力机制都是与循环网络结合使用的。

# 2	背景

关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离而增长.

在Transformer中，这被减少为一个常数级的操作，尽管代价是由于平均注意力加权位置而降低了有效分辨率，我们用Multi-Head Attention抵消了这一影响。

自注意力机制，是一种将单个序列的不同位置联系起来的注意力机制，以便计算序列的表示。

# 3	模型架构

编码器映射一个由符号表示的输入序列$(x_1,\dots,x_n)$到连续表示序列$z=(z_1,\dots,z_n)$。给定$z$，解码器生成一个符号输出序列$(y_1,\dots,y_n)$(一次生成一个元素)。在每一步中，模型都是自回归的，并在生成下一步时，将先前生成的符号作为额外的输入。

Transformer遵循这种总体架构，为编码器和解码器使用了自注意力堆栈和按点的全连接层，分别如图1的左右两部分所示。

<img src="D:%5CTypora%5Cuser-image%5C1677936891583.png" alt="1677936891583" style="zoom:67%;" />

## 3.1	编码器和解码器堆栈

**编码器：**编码器由具有$N=6$个相同层的堆栈组成。每层有两个子层。第一个是一个多头自注意力机制，第二个是一个简单的、按位置全连接的前馈网络。在`每两个子层`周围使用一个残余连接，然后进行层归一化。每个子层的输出是$LayerNorm(x+Sublayer(x))$，其中$Sublayer(x)$是子层本身实现的函数。为了方便这些残余连接，模型中的所有子层以及embedding层都会产生维度为$d_{model}=512$的输出。

**解码器：**解码器也由N = 6个相同层的堆栈组成。除了每个编码器层中的两个子层外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意力操作。与编码器类似，在`每个子层`周围使用残余连接，然后进行层归一化。我们还修改了解码器堆栈中的自注意力子层，以防止position关注后续position。这种掩蔽，结合输出embedding被一个position抵消的事实，确保对位置$i$的预测只能依赖于小于$i$位置的已知输出。

`图1的左半部分为编码层，右半部分为解码层`

## 3.2	Attention

注意力函数可以描述为将查询和一组键-值对映射到输出，其中查询、键、值和输出都是向量。输出是按值的加权和计算的，其中分配给每个值的权重是通过查询与相应键的兼容性函数计算的。

### 3.2.1	Scaled Dot-Product Attention

![1678014183771](D:%5CTypora%5Cuser-image%5C1678014183771.png)

输入由维度为$d_k$的查询和键以及维度为$d_v$的值组成。计算查询的所有键的点积，每个键除以$\sqrt{d_k}$，然后使用一个softmax函数来获得值的权重。

在实验中，同时计算一组查询上的注意力函数，将它们打包成一个矩阵$Q$。键和值对应的矩阵为$K$和$V$。计算输出矩阵为：
$$
\tag{1}
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$
最常用的两个注意函数是`加性注意`和`点积注意`(乘性注意)。

加性注意算法采用带有单一隐藏层的前馈网络计算兼容性函数。点积注意在实践中要更快、更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。

对于$d_k$值较大时，点积的大小会变大，将softmax函数推到具有极小梯度的区域，为了抵消这种影响，将点积乘以$\frac{1}{\sqrt{d_k}}$。

### 3.2.2	Multi-Head Attention

Multi-head attention使模型能够同时关注来自不同位置的不同表示子空间的信息。如果只有一个注意力头，平均会抑制这一点。
$$
MultiHead(Q,K,V)=Concat(head_1,\dots,head_h)W^O
\\
where \ head_i=Attention(QW^Q_i,KW^K_i, VW^V_i)
$$
其中，投影是参数矩阵$W^Q_i\in R^{d_{model}}$，，和