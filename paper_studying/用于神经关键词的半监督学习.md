# 摘要

我们研究生成关键短语的问题，这些关键短语总结了给定文档的关键点。虽然序列到序列(sequence-to-sequence, seq2seq)模型在这项任务上取得了显著的性能，但模型训练往往依赖于大量的标记数据，这只适用于资源丰富的领域。

在本文中，我们提出了利用标记数据和大规模无标记样本进行学习的`半监督关键词生成方法`。提出了两种策略：

首先，使用无监督关键词提取方法或自学习算法获得的`合成关键词`去标记未标记文档，然后与标记样本`结合`进行训练。

此外，我们研究了一个多任务学习框架，以联合学习生成关键词以及文章的标题。实验结果表明，我们基于半监督学习的方法优于只用标记数据训练的最先进的模型。

# 一、介绍

基于提取的方法无法生成源文档中没有出现的关键字，这些关键字通常是由人工注释器生成的，如图1所示。

![1673748201553](D:%5CTypora%5Cuser-image%5C1673748201553.png)

虽然seq2seq模型在关键词生成方面表现出良好的性能，但它严重依赖于大量的标记数据进行模型训练，这对于新域通常是不可用的。为了克服这一缺点，在这项工作中，我们通过利用丰富的未标记文档和有限的标记数据，研究了用于关键短语生成的半监督学习。直观地说，附加文档虽然没有标记，但可以提供关于通用语言特征和语篇结构的有用知识，例如关键短语的上下文信息，以及关键短语可能是名词短语或主要动词。

两种利用未标记数据的主要方法：

1. 第一种方法，首先用合成关键字标记未标记的文档，然后与标记数据混合进行模型预训练。合成关键词是通过现有的无监督关键词提取方法(例如，TF-IDF或TextRank)或自学习算法获得的。预训练的模型将仅在标记的数据上进一步微调。
2. 第二种方法，提出了一种多任务学习(MTL)框架。通过联合学习基于标记样本的关键短语生成的主要任务，以及基于未标记文档的标题生成的辅助任务。这里两个任务共享一个编码器。

即使有大量标记数据可用，使用未标记样本进行训练也可以进一步产生性能增益。

# 二、相关工作

# 三、神经关键词生成模型

在本节中，描述了建立在序列到序列(seq2seq)模型上的神经关键字生成模型，如图2所示。

![1673750717819](D:%5CTypora%5Cuser-image%5C1673750717819.png)

输入原文档作为一个序列：$x=x_1\cdots x_{|x|}$；其对应的关键词集合为$a=\{a_i\}^{|a|}_{i=1}$，$a_i$是一个关键词。

## 1、关键词序列公式

输入文档与每一个关键词$a_i$配对组成一个训练样本。将a中的关键词连接成一个关键词序列$y = a_1♦a_2♦···♦a_{|a|}$，其中$♦$是分隔关键短语(关键词)的分段。通过这种设置，seq2seq模型能够在一个序列中生成所有可能的关键短语，并从同一序列中捕获关键短语之间的上下文信息。

## 2、Seq2Seq注意力模型

使用源文档x及其关键字序列y，编码器将x编码为上下文向量，然后由解码器生成y。将编码器设置为单层双向LSTM模型，解码器设置为另一个单层LSTM模型。生成目标序列$p(y|x)$的概率公式为：

![1673832535556](D:%5CTypora%5Cuser-image%5C1673832535556.png)

其中 ，$y_{<t}=y_1y_2\cdots y_{t-1}$。

`具体公式看论文。`

运用全局注意力机制计算上下文向量：

![1673832924818](D:%5CTypora%5Cuser-image%5C1673832924818.png)

其中，$\alpha_{t,i}$是注意力权重；$W_{att}$包含可学习参数。在本文中，我们省略了偏差变量以节省空间。

解码器在时间步长t时预测$y_t$的概率分解为：

![1673833412229](D:%5CTypora%5Cuser-image%5C1673833412229.png)

其中，$f_{softmax}$是softmax函数，$W_{d_1}$和$W_{d_2}$是可学习参数。

## 3、Pointer-generator网络(指针生成器网络)

通过指针生成器网络利用复制机制，允许解码器直接从输入文档复制单词，从而缓解词汇量不足(OOV)问题。在时间步t时，计算生成概率$p_{gen}$为：

![1673834049395](D:%5CTypora%5Cuser-image%5C1673834049395.png)

其中，$f_{sigmoid}$是sigmoid函数；$W_c$，$W_s$和$W_y$是可学习参数。$p_{gen}$扮演切换器的角色，用概率$p_{vocab}$从固定的词汇表中选择生成一个单词，或者用注意力分布$\alpha_t$直接从源文档中复制一个单词。通过固定词汇表和扩展源文档词汇表的组合，预测$y_t$的概率为：

![1673834729642](D:%5CTypora%5Cuser-image%5C1673834729642.png)

其中，如果$y_t$没有出现在固定词汇中，则第一项为0；如果$y_t$在源文档之外，第二项就是0。

## 4、监督学习

带有标签的数据集$D_p=\{x_{(i)},y_{(i)}\}_{i=1}^N$，seq2seq模型的损失函数如下：

![1673835747355](D:%5CTypora%5Cuser-image%5C1673835747355.png)

其中，$\theta$包含所有的模型参数。

## 5、关键词推理与评级策略

使用集束搜索进行解码，并利用前R关键字序列来产生最终的关键字。这里集束尺寸为50，R为50。

提出了一个评级策略来收集最终的关键词集合。具体来说，按顺序从排名最高的光束到排名较低的光束收集唯一的关键字，并按照生成过程中相同顺序对关键字进行排序。直观地说，排序越高的序列质量可能越好。对于相同序列的关键字，更突出的关键字通常是最先生成的。算法1给出了排序方法。

![1673854275760](D:%5CTypora%5Cuser-image%5C1673854275760.png)

# 四、用于关键词生成的半监督学习

如图3所示，提出了两种方法来利用丰富的未标记数据。

![1674091360015](D:%5CTypora%5Cuser-image%5C1674091360015.png)

第一种是使用无监督关键词提取方法或自学习算法提供合成关键词，然后与标记数据混合进行模型训练；

第二种是多任务学习，联合生成关键词和文档标题。

将大量未标记的文档看作$D_u=\{ x'_{(i)}\}^M_{i=1}$，已经标记的数据看作$D_p=\{ x_{(i)},y_{(i)}\}^N_{i=1}$，其中$M\gg N$。

## 1、合成关键词构造

第一种提出的技术是通过为未标记的文档分配关键字来构造合成标记数据，然后将合成数据与人工标记数据混合用于建模训练。

直观地说，添加合成关键字的训练样本有两个潜在的好处：

1. 编码器在训练中暴露在更多的文档中；
2. 解码器也从识别关键字上下文信息的额外信息中受益。

**无监督学习方法。**这里选择TF-IDF和TextRank。将这两种方法结合成一种混合方法，首先采用这两种方法分别从文档中选择前K个关键词，然后采用去除重复的并集。为了构建关键词序列，将TF-IDF和TextRank中的术语按照相应的排序顺序连接起来。在实验中设K为5。

**自学习算法。**首先通过在标记语料库$D_p$上训练seq2seq模型来构建一个基线模型。然后利用训练好的基线模型在给定未标记文档$x’$的情况下生成合成关键字序列$y'$。采用集束搜索来生成合成关键词序列，集束大小设置为10。顶部的一束被选中。

**训练过程。**获得合成数据后，将标记数据和合成数据混合起来进行训练seq2seq模型。如算法2所述。根据验证集选择性能最好的模型，并微调标记数据直至收敛。![1674095438096](D:%5CTypora%5Cuser-image%5C1674095438096.png)

## 2、辅助任务的多任务学习

第二种利用无标签文档的方法是采用多任务学习框架，该框架通过参数共享策略将关键字生成的主要任务与辅助任务相结合。与文中的模型结构类似，主任务和辅助任务共享一个编码器网络，但有`不同的解码器`。多任务学习将受益于源端信息，提高编码器模型的通用性。

选择`标题生成`作为辅助任务。令$D_u'=\{ x'_{(i)},q_{(i)}\}^M_{i=1}$表示为未标记数据库$D_u$分配标题的数据集。多任务学习的损失函数分解为：![1674096792629](D:%5CTypora%5Cuser-image%5C1674096792629.png)

其中，$\theta^e$表示编码器参数；$\theta_1^d$和$\theta_2^d$是解码器参数。

**训练过程。**采用简单的交替训练策略，在主任务和辅助任务之间切换训练。具体来说，首先在一次迭代中用$D'_u$估计辅助任务参数，然后在T个迭代中用$D_p$(标记数据集)在主任务上训练模型。多次遵循这个训练过程，直到主要任务的模型收敛。设T为3。

# 五、实验

## 1、数据集

数据集：科学文章

![1674179233337](D:%5CTypora%5Cuser-image%5C1674179233337.png)

## 2、实验设置

**数据预处理。**文本首先由NLTK标记并小写，然后将数字替换为<digit>。设置源文本的最大长度为200，目标文本为40。编码器和解码器的词汇量都是50K。单词嵌入大小设置为128。在训练过程中随机初始化和学习嵌入。隐藏向量的大小为512。dropout率为0.3，最大梯度归一化是2，采用Adagrad训练模型，学习率为0.15，初始累加器速率为0.1。

**构造合成数据。**64batch进行模型预训练，然后减少到32用于模型微调。对于两个训练阶段，在8次迭代后，学习率都以0.5的速率下降。对于多任务学习，批大小设置为32个，20个训练迭代后学习率降低一半。为了建立seq2seq基线模型，我们将批大小设置为32，并在8个epoch后降低学习率。对于自学习算法，将集束大小设置为10，对未标记的数据生成目标序列，并保留最上面的序列。

## 3、与基线的比较

肯定比基线强。

## 4、合成关键词的质量的影响

两组实验：一组用于评估无监督学习，另一组用于自学习算法。

对于自学习算法，使用以下选项来生成合成关键词：

1. 集束-尺寸-3：在使用标记数据训练的基线模型的基础上，使用较小的集束大小为3的集束搜索来生成合成数据。
2. 训练模型：用经过自学习算法训练的模型，在40K有标签数据和400K无标签数据上生成集束大小为10的前一个关键字序列。

对于无监督学习方法，我们最初将TF-IDF和TextRank中的top-K (K = 5)关键字合并，这里我们使用K设置为1或10的选项来提取关键字：

1. Top@1：使用TF-IDF或TextRank，我们只保留前1个提取，然后取两者的并集。
2. Top@10：我们从TF-IDF或TextRank中保留前10个提取的术语，然后取并集。

如图4所示，当模型使用更好质量的合成关键字进行预训练时，“训练模型”的结果始终能产生更好的性能(即更低的困惑度)。

![1674180781207](D:%5CTypora%5Cuser-image%5C1674180781207.png)

此外，在对模型进行预训练并进行微调(调整标记数据)之后，图5中的结果表明，基线之间的差异变得不显著——合成关键字的质量对最终分数的影响有限。

![1674180888640](D:%5CTypora%5Cuser-image%5C1674180888640.png)

在对标记数据进行微调后，解码器获得了额外的知识，从而导致更好的性能和最小的选项之间的差异。

## 5、未标记数据的数量的影响

利用更多的未标记数据进行模型训练可以提高模型性能。

## 6、跨域测试的初步研究

原本实验使用的是科学文章，使用了新闻文章进行跨域研究。

实验结果见表4。

![1674181143311](D:%5CTypora%5Cuser-image%5C1674181143311.png)

结论：学习了两个领域之间的普遍特征，所以有用。

## 7、大规模标记数据训练

在一个更大的有130K对标记的数据集上进行实验，以及400K未标记的数据集。这里，基线seq2seq模型建立在130K数据集上。

从表5中目前的关键词生成结果可以看出，在大规模标记数据集上，未标记数据仍然有助于模型训练。

![1674181283560](D:%5CTypora%5Cuser-image%5C1674181283560.png)

从表6所示的缺失关键词生成结果来看，半监督学习也提高了分数。

![1674181341771](D:%5CTypora%5Cuser-image%5C1674181341771.png)

# 六、结论

在本文中，提出了一种半监督学习框架，利用无标记数据来生成基于seq2seq模型的关键词。引入了合成关键词构造算法和多任务学习，有效地利用了大量的无标签文档。大量的实验证明了提出的方法的有效性，即使在有大规模标记数据可用的情况下。