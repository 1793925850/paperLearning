# 摘要

**目前存在的问题：**现有的大多数基于图的模型使用`共现`链接作为`内聚`指标来模拟语法元素之间的关系。但是，一个单词在文档中可能有不同的表达形式，也可能有几个同义词。仅仅使用协同发生信息无法捕获此信息。

在本文中，利用词嵌入作为上下文，在词间图中添加`语义信息`，增强了基于图的排名模型。

实证结果表明，词嵌入邻域信息提高了模型的性能。

# 一、介绍

基于图的排名模型的基本思想是“投票”或“推荐”。

一般步骤如下：

1. 首先，根据文档内的信息构造字间图(也叫候选图)；
2. 然后，运行一些基于图表的排名模型，如PageRank或HITS来计算每个单词的得分；
3. 最后，根据单词和短语的计算得分选择关键短语。

**统计信息和语义信息都可以帮助改进关键词检测模型，但关键问题是如何将两者结合起来。**

语义信息可以帮助识别密切相关的项，可用于`细化加权图`。例如，一个概念可能会在文档中以不同的表达方式出现多次，即使是基础单词也可能在语义上密切相关。

本文提出了一种利用词嵌入作为背景语义信息来提高基于图的排名模型性能的方法。模型将文档表示为一个加权图，其中顶点是短语，边度量它们之间的连接强度。首先计算在一定窗口大小内短语的同时出现次数。然后，利用词嵌入算法计算顶点间的边权值。与以前的方法不同，利用词嵌入的邻域信息，并且发现它在捕获潜在的语义相关性方面非常有效。

这里的基本直觉是:假设在图中，短语A与单词B、C和D同时出现。如果B和C在语义上相关，(A, B)和(A, C)之间的联系应该更强。

利用这种语义相关性来构造加权图。然后，采用偏PageRank方法来根据它们的PageRank分数对短语进行排名。最后，提出了一种集成方法来获得输出关键字列表。

# 二、相关工作

候选短语得分是每个单词得分的总和。

# 三、提出的方法

## 1、概述

在基于图表的排名模型中，我们通常使用一定窗口大小内的共现率作为单词/短语之间的内聚指标。两个单词/短语同时出现的次数越多，这两个单词之间的联系就越强，它们就越有可能获得更高的PageRank得分。

但是，一个概念可以用不同的形式表达，使用无语义的共现会导致信息丢失。例如，假设短语A与短语B、C、D各出现一次，我们有背景知识，B的语义与C非常接近。在共现图上运行PageRank, B、C和D的得分是相同的。但是，直觉上B和C应该得到更高的分数，A与B和C所代表的概念之间的联系应该更强。

提出的方法就是为了解决这个问题。对于给定边AB的权值，我们不仅考虑了a与B的共现，还考虑了a的邻域集与B的共现，以及B的邻域集与a的共现。借助这个邻域集，我们可以加强短语间的关系，这是不能通过单独使用统计信息就能捕获的。

![1671849630570](D:%5CTypora%5Cuser-image%5C1671849630570.png)

算法1中显示了我们的关键词提取方法的高级大纲。下面给出了各个步骤的详细信息。

## 2、候选短语

首先提取候选短语作为加权图的顶点，它们根据以下模式从文档中提取：`(形容词)∗(名词)+`，这意味着零或多个形容词后面至少有一个名词。

## 3、词嵌入

词嵌入将词语投射到向量空间中，以便于提取潜在的相似性或语义。像词袋这样的模型倾向于给每个单词一个独热编码的表示，这受到维数问题的困扰，因为词汇量可能非常大。

另一方面，Word2Vec是一种基于神经网络的模型，它使用邻域上下文来捕获单词的分布，并已被证明在捕获单词的潜在语义方面是有效的。

在这项工作中，我们使用预先训练好的来自谷GoogleNews的1000亿个单词的Word2Vec模型作为背景信息来计算单词相似度(嵌入维数为300)。

从词嵌入到短语嵌入，对于某一个短语，我们将给定短语内所有的词嵌入得分相加作为短语嵌入值。

## 4、邻域构建

对于给定的短语$v_0$，我们找到与该短语语义相关的k个最近邻居。$v_i$和$v_j$之间的相似度是用这两个短语在它们的嵌入空间中的余弦相似度来计算的：

![1671929669650](D:%5CTypora%5Cuser-image%5C1671929669650.png)

基于阈值构造$v_0$的邻域集，余弦相似度超过一定阈值将加入到$v_0$的邻域集中。余弦相似度也可以作为短语之间语义联系的置信值。

## 5、基于图的排名

### 5.1、图构建

对于一个给定的文档，使用无向图$G=(V,E)$来构建短语之间的关系的模型。每一个顶点为一个候选短语，它被限制在一定的模式。使用特定窗口大小w内的共现次数来表示短语之间的内聚关系。

对于短语$v_i$和$v_j$之间的边$e_{ij}$，不仅考虑短语$v_i$和$v_j$之间的共现，还考虑短语$v_i$和$v_j$的邻域集中短语的共现，以及短语$v_j$和$v_i$的邻域集中短语的共现：

![1671930814832](D:%5CTypora%5Cuser-image%5C1671930814832.png)

其中，$count(v_i,v_k)$是短语$v_i$和$v_k$共现的次数，$D_j$是短语$v_j$的邻域集。

### 5.2、有偏置的PageRank

接下来，用有偏置的PageRank算法来计算每个顶点的分数。设$C(v_i)$表示与$v_i$相连的顶点集合，则计算$v_i$的得分为：

![1671931371518](D:%5CTypora%5Cuser-image%5C1671931371518.png)

其中，$w(v_j,v_i)$表示顶点$v_j$和$v_i$之间的权重。参数d为阻尼因子，通常取0.85。$r^*_i$是随机跳跃向量。

![1671932161767](D:%5CTypora%5Cuser-image%5C1671932161767.png)

其中，$\hat{r_k}$是$v_i$出现的句子的逆位置的和。

 直观的感觉是，在文档中出现频率较高和较早的短语往往更重要。 

在实验中，递归地计算顶点分数，直到两个连续迭代之间的差异很小(例如小于0.0001)或达到最大迭代次数(例如100)。

## 6、排名集成

计算出顶点的PageRank分数后，我们可以根据候选短语的PageRank分数进行递减排序。

首先，提出了一种朴素的方法，将提出的模型与TF-IDF结合起来，一般的步骤是：

1. 选择每个模型的前25个候选短语来构建候选列表$l_a$和$l_b$；
2. 关键短语应该在两种方法的候选列表中；
3. 如果一个单词短语作为另一个多单词短语的一部分出现在候选列表中，那么过滤掉这个单个单词短语；
4. 每个候选列表的相对排名不变。

伪代码如下：

![1671933797098](D:%5CTypora%5Cuser-image%5C1671933797098.png)

此外，还采用了Kemeny-Young秩聚合方法，该方法最小化排序得分的kendall-tau总距离。

最后，在排名聚合之后，我们输出组合候选列表中排名前m的短语作为关键短语，其中m等于每个文档的平均关键短语。

# 四、实验设置

![1671935702661](D:%5CTypora%5Cuser-image%5C1671935702661.png)

# 五、结果和讨论

## 1、性能比较

表II显示了我们的模型在inspect、DUC2001和Nguyen数据集上的精度、召回率和F-score值。

![1671935840714](D:%5CTypora%5Cuser-image%5C1671935840714.png)

OurModel(1)表示使用我们自己设计的组合方法结合Tf-idf的邻域信息模型。OurModel(2)表示邻域信息模型Tf-idf和PositionRank的Kemeny-Y oung秩聚合。

此外，可以发现，当捕获数据不同方面的几个不同的基本方法聚合在一起时，集成方法更强大。

此外，我们的方法的一个优点是，除了预训练的Word2Vec嵌入之外，它并不严重依赖外部资源。

## 2、结论

本文提出了一种新的关键词提取方法，该方法利用词嵌入在词之间添加潜在的语义链接，从而能够为文档创建更准确的加权图。本文还提出了一种集成的方法来聚合一些现有的模型，这可以帮助我们获得更好的结果。