# 摘要

本研究的目的是在摘要中使用**双向编码器表示**(Bidirectional Encoder Representations from Transformers，BERT)方法实现一个关键字搜索系统，作为一个抽象词搜索系统，**根据期望的关键字结果产生具有上下文的关键字答案**。本研究的结果在epoch(循环次数) 500得到了最优的结果，并从这些结果中使用了一个包含444个训练数据和10个测试数据的数据集进行测试。

# 一、介绍

本研究实现了BERT方法在科技文章摘要关键词搜索中的应用。

# 二、相关工作

# 三、材料和方法

## 1、数据集

**semaval 2010**

在数据集中，有两种不同的文件类型：

- 第一种是期刊摘要的集合；(X)
- 科学数据和第二个数据是以关键词的形式从摘要中提取的。(Y)

总共使用的科学期刊数据为564种期刊，其中将分为444个训练数据(400个为训练数据，44个为验证数据)和10种测试数据，其中测试数据取自IEEE期刊。

## 2、BERT

BERT是一种预训练的语言模型，它由一组transformer编码器组成，这些编码器在单词和句子级别上表示文本，并借助掩码语言建模和下一句预测等无监督训练技术。作为一个预训练模型，BERT训练了3300万个单词。BERT使用带有注意机制的transformer编码器来学习单词之间的上下文关系。

Transformer的原生形式包括**编码器**和**解码器**，其中**编码器学习文本输入**，**解码器被调优以执行特定任务**。作为一种语言模型，理解输入文本是唯一重要的因素。由于这个原因，BERT只使用transformer编码器。不像各种方向模型那样按顺序读取文本输入，transformer编码器一次读取整个单词序列。

BERT基本模型可以通过简单地在上面添加一个softmax分类层来对文本分类进行微调，因此它将预测给定文本序列的类别。

公式q：
$$
\tag{1}p(c|H)=softmax(WH)
$$

$$
\tag{2}Z=softmax(Qx\frac{K^T}{\sqrt{d_k}}V)
$$

其中，Q是查询矩阵，K是键矩阵，V是值矩阵，$d_k$是键向量的维度大小，Z是单头注意力得分矩阵。为每个注意头生成注意得分矩阵，将得分矩阵进行组合，然后对前馈层的每个输入维进行约简。



基于注意力的系统模型设计：

![image-20230619145553594](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202306191455635.png)

### 2.1、微调BERT

### 2.2、进一步的预微调

进一步使用屏蔽语言模型和针对特定领域结果的下一句预测任务来预训练BERT。另外还采用了三种预训练方法：

- **任务内预训练**，在目标任务的训练数据上对BERT进行进一步的预训练；
- **域内预训练**，预训练数据来源于目标任务的同一域。例如，有许多不同的情感分类任务具有相似的数据分布。我们将在这些任务的累积训练数据上进一步预训练BERT；
- **跨域预训练**，即从目标任务的相同域和不同域获取预训练数据。

### 2.3、微调BERT

# 四、结果和讨论

![image-20230619151630543](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202306191516572.png)

![image-20230619151741254](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202306191517276.png)

![image-20230619151847579](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202306191518607.png)

# 五、结论













