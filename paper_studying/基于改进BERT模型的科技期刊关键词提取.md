# 摘要

本研究提出了一种基于改进BERT模型的科技期刊关键词提取方法。

该研究方法改进了集合的整体相似度度量，引入**复合关键词密度**，结合**分词**、**词义集距离**和**密度聚类**构建改进的BERT框架，建立了基于**I-BERT框架**的复合关键词热分析模型。

以中国知网(CNKI) 2017-2019年收录的21种社科管理期刊14420篇文章为实验数据，通过**词间距**、**类间距**、**提取准确率**、**热关键词召回率**等数据验证了所提方法的优势。

本文提出的方法可以保证**科技期刊**在捕捉本学科热点话题时的时效性和准确性。

# 一、介绍

目前在期刊策划中，可以利用大数据技术实现数据挖掘，利用深度学习工具发现研究热点，实现期刊动态的智能化管理。

# 二、研究现状

本文以BERT模型作为词向量获取的基础，将文档复合关键词划分为词向量集合表达式，并在集合中保留复合词向量；同时，创新地定义了集合相似度度量方法，使各元素信息能够独立参与度量，并保留了复合词向量的复合特征。最后，构建了改进的BERT (i-BERT)复合关键词流行度分析模型。

根据关键词的顺序和频率中心得到热门关键词。

# 三、模型构建

为了使关键词的描述更加准确，所涉及的关键词往往是由几个简单的词组成的**复合短语**，如坐标短语、从属短语、主谓短语、动宾短语等。

为了获得这类短语的完整向量，采用BERT模型的句子向量获取方法，可以获得任意长度的词向量。

考虑到复合词组合时单字具有信息独立性，本文进一步对复合词进行拆分，使用**包含单字和复合词的多元素语义集**作为复合词的**词义表达**。

集合中元素的向量采用BERT词向量模型，并在相似度度量中加强了一致信息的权重，使得复合词的聚合不再均匀，同义词相互吸引。最后实现类中关键词的自生成标签，使生成的中心词对类具有一定的通用性，并保留了类的原有信息。建立模型的过程如图1所示。

![image-20230620170312295](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202306201703396.png)

步骤如下：

1. 对样本集第i个月的所有期刊样本集的所有j个复合关键字集合$mk_i=\{cw_1,cw_2,\dots,cw_j \}$进行分词，得到**模糊词义集**。比如：关键词$cw_j$=‘AB’可以被分割成一个模糊词义集$\{A,B,AB\}$；
2. 基于BERT模型得到集合中每个元素的向量表达式(也就是每个词所对应的embedding)，使得每个复合词可以进一步表示为一个拆分的词义集合$cw=\{v_1,v_2,\dots,v_k\}$；
3. **确定复合词集距离度量方法**，将CFSFDP词密度聚类与密度距离决策图相结合，得到期刊关键词类；
4. 对类中的关键词进行拆分，得到被分割词集，识别每个集中被分割词集的密度中心，并以该集中的分布散度作为置信度来决定是否使用中心词。对所有保留的中心词进行拼接，得到关键字集的中心词。

> 关键字集中的每个关键字都可以看作是一个**属性**，出现频率最高的关键字作为一个**目标**。从而计算出不同关键字在该关键词中出现频率最高的概率，即该关键字对目标影响权重的度量。
>
> 利用**最大信息系数**可以解决**分词集**(这里的分词集指的是分到一类的那一群关键词)中属性与目标之间的关系。通过计算互信息和熵来估计不同属性值对目标的影响程度，并基于计算函数依赖权值。

设X为随机得到的中心词。X的熵可以用公式1表示：
$$
\tag{1}
H(X)=-\sum_{x-X}p(x)logP(X)
$$
在式1中，$H(X)$表示信息量，$P(X)$表示影响的概率。$P(X)$越大，$H(X)$越大。

在关键词集中，将两个关键词作为一组。**不同关键词对彼此的临界程度**如公式2所示：
$$
\tag{2}H(X|Y)=\sum_{y-Y}P(y)\sum_{x-X}P(x|y)logP(x|y)
$$
在公式2中，H(X│Y)表示当X有一定程度的影响时，Y存在的概率。

同时，Y也会给X带来一定的**损失值**，如公式3所示：
$$
\tag{3}I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)
$$
公式4源于公式2和公式3：
$$
\tag{4}0\le I(X;Y)\le min\{H(X),H(Y)\}
$$
从而可以得到不同关键词在关键词中频繁出现的概率，即最大信息系数为：
$$
\tag{5}
SU_{MAX}(X;Y)=2[\frac{I_{MAX}(X;Y)}{H(X_i)+H(Y)}]
$$

## 1、获得词向量

例如，关键词“ABC”可以划分为{A，B，C，AB，BC，ABC}。最后使用BERT模型将集合中的每个单词转换为单词向量格式。

BERT模型的重要部分是基于双向transformer编码器实现的，模型结构如图2所示：

![image-20230621133658170](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202306211336249.png)

其中，$E_1,E_2,\dots,E_n$表示文本输入，通过双向transformer编码器得到文本的向量化表示，即主要通过transformer编码器实现文本的embedding化。

BERT模型的输出有两种形式:一种是字符级向量，即输入短文本的每个字符都有一个向量表示；另一个是句子级向量，即BERT模型输出的最左边的[CLS]特殊符号向量，它认为这个向量可以表示整个句子的语义。

本文采用**句子级向量**输出方法。为每个复合词集得到embedding$cw_j=\{v_{j1},v_{j2},\dots,v_{jk}\}$。

## 2、基于改进的BERT模型的关键词提取

通过实验发现，在基于传统BERT模型得到的词向量中，两个词的相似度可以分为[相同、非常相似、一般、完全不相似]，对应的欧氏距离集为[0,7.6,10,13.5]，标记为$[0,\gamma_1,\gamma_2,\gamma_3]$。但这并不合理。

为了解决这一问题，本文对传统BERT模型的距离进行了优化。本文采用集合中每对匹配元素的距离权值的归一化非线性映射，使得相似词的权值增大，不相关词的权值减小。本文还建立了一种改进的BERT (i-BERT)模型，用于组合关键词的流行度分析，从而达到校正组合关键词集距离的目的。

**复合关键词集距离优化**：

1. 计算两个复合词集$cw_1=\{v_{11},v_{12},\dots,v_{1k}\}$和$cw_2=\{v_{21},v_{22},\dots,v_{2k}\}$的相似度，可以理解为两个集合的元素在加权下的双向最近邻映射距离之和。两个复合词集的距离可以定义为：
