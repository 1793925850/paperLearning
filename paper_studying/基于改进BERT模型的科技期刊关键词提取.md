# 摘要

本研究提出了一种基于改进BERT模型的科技期刊关键词提取方法。

该研究方法改进了集合的整体相似度度量，引入**复合关键词密度**，结合**分词**、**词义集距离**和**密度聚类**构建改进的BERT框架，建立了基于**I-BERT框架**的复合关键词热分析模型。

以中国知网(CNKI) 2017-2019年收录的21种社科管理期刊14420篇文章为实验数据，通过**词间距**、**类间距**、**提取准确率**、**热关键词召回率**等数据验证了所提方法的优势。

本文提出的方法可以保证**科技期刊**在捕捉本学科热点话题时的时效性和准确性。

# 一、介绍

目前在期刊策划中，可以利用大数据技术实现数据挖掘，利用深度学习工具发现研究热点，实现期刊动态的智能化管理。

# 二、研究现状

本文以BERT模型作为词向量获取的基础，将文档复合关键词划分为词向量集合表达式，并在集合中保留复合词向量；同时，创新地定义了集合相似度度量方法，使各元素信息能够独立参与度量，并保留了复合词向量的复合特征。最后，构建了改进的BERT (i-BERT)复合关键词流行度分析模型。

根据关键词的顺序和频率中心得到热门关键词。

# 三、模型构建

为了使关键词的描述更加准确，所涉及的关键词往往是由几个简单的词组成的**复合短语**，如坐标短语、从属短语、主谓短语、动宾短语等。

为了获得这类短语的完整向量，采用BERT模型的句子向量获取方法，可以获得任意长度的词向量。

考虑到复合词组合时单字具有信息独立性，本文进一步对复合词进行拆分，使用**包含单字和复合词的多元素语义集**作为复合词的**词义表达**。

集合中元素的向量采用BERT词向量模型，并在相似度度量中加强了一致信息的权重，使得复合词的聚合不再均匀，同义词相互吸引。最后实现类中关键词的自生成标签，使生成的中心词对类具有一定的通用性，并保留了类的原有信息。建立模型的过程如图1所示。

![image-20230620170312295](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202306201703396.png)

步骤如下：

1. 对样本集第i个月的所有期刊样本集的所有j个复合关键字集合$mk_i=\{cw_1,cw_2,\dots,cw_j \}$进行分词，得到**模糊词义集**。比如：关键词$cw_j$=‘AB’可以被分割成一个模糊词义集$\{A,B,AB\}$；
2. 基于BERT模型得到集合中每个元素的向量表达式(也就是每个词所对应的embedding)，使得每个复合词可以进一步表示为一个拆分的词义集合$cw=\{v_1,v_2,\dots,v_k\}$；
3. **确定复合词集距离度量方法**，将CFSFDP词密度聚类与密度距离决策图相结合，得到期刊关键词类；
4. 对类中的关键词进行拆分，得到被分割词集，识别每个集中被分割词集的密度中心，并以该集中的分布散度作为置信度来决定是否使用中心词。对所有保留的中心词进行拼接，得到关键字集的中心词。

> 关键字集中的每个关键字都可以看作是一个**属性**，出现频率最高的关键字作为一个**目标**。从而计算出不同关键字在该关键词中出现频率最高的概率，即该关键字对目标影响权重的度量。
>
> 利用**最大信息系数**可以解决**分词集**(这里的分词集指的是分到一类的那一群关键词)中属性与目标之间的关系。通过计算互信息和熵来估计不同属性值对目标的影响程度，并基于计算函数依赖权值。

设X为随机得到的中心词。X的熵可以用公式1表示：
$$
\tag{1}
H(X)=-\sum_{x-X}p(x)logP(X)
$$
在式1中，$H(X)$表示信息量，$P(X)$表示影响的概率。$P(X)$越大，$H(X)$越大。

在关键词集中，将两个关键词作为一组。**不同关键词对彼此的临界程度**如公式2所示：
$$
\tag{2}H(X|Y)=\sum_{y-Y}P(y)\sum_{x-X}P(x|y)logP(x|y)
$$
在公式2中，H(X│Y)表示当X有一定程度的影响时，Y存在的概率。

同时，Y也会给X带来一定的**损失值**，如公式3所示：
$$
\tag{3}I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)
$$
公式4源于公式2和公式3：
$$
\tag{4}0\le I(X;Y)\le min\{H(X),H(Y)\}
$$
从而可以得到不同关键词在关键词中频繁出现的概率，即最大信息系数为：
$$
\tag{5}
SU_{MAX}(X;Y)=2[\frac{I_{MAX}(X;Y)}{H(X_i)+H(Y)}]
$$

## 1、获得词向量

例如，关键词“ABC”可以划分为{A，B，C，AB，BC，ABC}。最后使用BERT模型将集合中的每个单词转换为单词向量格式。

BERT模型的重要部分是基于双向transformer编码器实现的，模型结构如图2所示：

![image-20230621133658170](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202306211336249.png)

其中，$E_1,E_2,\dots,E_n$表示文本输入，通过双向transformer编码器得到文本的向量化表示，即主要通过transformer编码器实现文本的embedding化。

BERT模型的输出有两种形式:一种是字符级向量，即输入短文本的每个字符都有一个向量表示；另一个是句子级向量，即BERT模型输出的最左边的[CLS]特殊符号向量，它认为这个向量可以表示整个句子的语义。

本文采用**句子级向量**输出方法。为每个复合词集得到embedding$cw_j=\{v_{j1},v_{j2},\dots,v_{jk}\}$。

## 2、基于改进的BERT模型的关键词提取

通过实验发现，在基于传统BERT模型得到的词向量中，两个词的相似度可以分为[相同、非常相似、一般、完全不相似]，对应的欧氏距离集为[0,7.6,10,13.5]，标记为$[0,\gamma_1,\gamma_2,\gamma_3]$。但这并不合理。

为了解决这一问题，本文对传统BERT模型的距离进行了优化。本文采用集合中每对匹配元素的距离权值的归一化非线性映射，使得相似词的权值增大，不相关词的权值减小。本文还建立了一种改进的BERT (i-BERT)模型，用于组合关键词的流行度分析，从而达到校正组合关键词集距离的目的。

**复合关键词集距离优化**：

1. 计算两个复合词集$cw_1=\{v_{11},v_{12},\dots,v_{1k}\}$和$cw_2=\{v_{21},v_{22},\dots,v_{2k}\}$的相似度，可以理解为两个集合的元素在加权下的双向最近邻映射距离之和。两个复合词集的距离可以定义为：
   $$
   \tag{6}
   Dis(cw_1,cw_2)=\frac{1}{2}[\sum_{i=1}^k\omega_{1,i}dis(v_{1,i},cw_2)+\sum_{j=1}^{k'}\omega_{2,j}dis(v_{2,j},cw_1)]
   $$
   其中：
   $$
   \tag{7}
   dis(v_{1,i},cw_2)=min[dis'(v_{1,i},v_{2,1}),dis'(v_{1,i},v_{2,2}),\cdots,dis'(v_{1,i},v_{2,k'})]
   $$

   $$
   \tag{8}
   dis'(v_{1,i},v_{2,1})=\frac{(dis''(v_{1,i},v_{2,1}))^2}{\gamma_1^2}
   $$

   上式中，$dis''(v_{1,i},v_{2,1})$是BERT模型中单词向量的欧氏距离。经过复合词集的距离校正后，将原欧氏距离$[0,7.6,10,13.5]$调整成标记为$[0,\gamma_1',\gamma_2',\gamma_3']$的$[0,7.6,10,13.5]$。

2. 为了将相似度低的关键词从密度分布中分离出来，需要对属性相似的关键词进行**聚合**，达到“**同义词相互吸引**”的聚合效果。本文选择正态分布函数作为定义权重ω的基础。标准差附近的显著变化也能起到非线性激活作用。权值$\omega_{1,i}'$被定义为：
   $$
   \tag{9}
   \omega_{1,i}'=\begin{cases}
      \frac{[e^{\frac{-(dis(v_{1,i},cw_2))^2}{2{\gamma_2'}^2}}-e^{\frac{-(\gamma_3')^2}{2{\gamma_2'}^2}}]}{2+e^{\frac{-(\gamma_3')^2}{2{\gamma_2'}^2}}} & 0<dis(v_{1,i},cw_2)\le\gamma_3' \\
      e^{\frac{-(\gamma_3')^2}{2{\gamma_2'}^2}} & \gamma_3'\le dis(v_{1,i},cw_2)
   \end{cases}
   $$
   导入数据后，权重变化曲线如图3所示：![image-20230623092833742](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202306230928794.png)

   对于集合中的多对词，归一化后得到的最终权值如下：
   $$
   \tag{10}
   \omega_{1,i}=\frac{\omega_{1,i}'}{\displaystyle\sum_{i=1}^k\omega_{1,i}'+\displaystyle\sum_{j=1}^{k'}\omega_{2,j}'}
   $$

3. 复合词密度的优化：首先要把一个复合关键字的词义集看作一个数据点，鉴于数据密度的性质，两个数据点之间的密度效应与距离呈负相关。每个数据点都有一个密度影响函数，所有数据点叠加得到最终的密度空间。定义复合词词义集$cw_i$的密度为：
   $$
   \tag{11}
   \rho_i=\displaystyle\sum_{j=1}^ne^{\frac{-Dis(cw_i,cw_j)^2}{2\sigma^2}}
   $$
   其中，标准差$\sigma$决定了正态分布窗口的大小。标准差的取值是对所有距离数据从小到大进行排序，将前2 ~ 10%的距离视为标准差，具有较好的密度识别效果。

   在**距离测量**中，实现了集合中不同信息元素权重的调整，这是内部匹配度极性的体现。在**密度测量**中，计算中的非线性是在距离确定后，将集合视为空间中分布的一个点，对非线性密度对外部分布的影响进行调整。

   本文主要利用**CFSFDP**算法本身的性质来设置阈值进行定性划分，将密度和距离较大的点分离出来作为**类中心**。在式12中，最近邻距离是指所有大于当前点密度的数据点到最近点的距离，可以定义为：
   $$
   \tag{12}
   \delta_i= \begin{cases}
      \displaystyle\max_j(Dis(cw_i,cw_j)) & \forall j,\rho_i>\rho_j \\
      \displaystyle\min_{j:\rho_j>\rho_i}(Dis(cw_i,cw_j)) & others
   \end{cases}
   $$

## 3、类标签生成

聚类完成后，需要对类中的关键字进行汇总，生成类标签。类标签生成过程如图4所示：

![image-20230625132156193](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202306251321264.png)

1. 统计当前类中复合词集中简单词的词频和位置顺序。判断简单词是否为近义词的依据为$dis'(v_{1,i},v_{2,1})<\gamma_1'$，当位置顺序均匀分布时取平均值；对位置顺序分布进行聚类时，取位置最集中的位置。假设一组简单的词序为$[x_1,x_2,\dots,x_p]$，其几何比例被映射到0.1~1的区间为$[x_1',x_2',\dots,x_p']$。合并$[x_1',x_2',\dots,x_p']$中相似的项，从最相似项的顺序开始，从大到小合并；
2. 从词频由大到小判断，第一个词为默认输出，第i个词是否输出取决于该词的词频是否大于设置的阈值η，以及该词频与前i-1个输出词的序差diff是否大于设置的阈值ε。
3. 根据统计rank值对输出的单词进行排序，并将其组合成类描述标签。

# 四、实证分析

## 1、词向量获取的效果分析



