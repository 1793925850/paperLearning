# 前言

统计语言建模的一个目标是学习语言中词(或单词)序列的联合概率函数。这在本质上是困难的，因为维度的诅咒:测试模型的词序列很可能与训练期间看到的所有词序列不同(意思就是：没见过这么说话的)。

> 维度诅咒：原以为增加更多的特征就能获得更好的学习效果，实际上增加到一定程度学习效果就会达到最好，再增加特征(也就是维度)就会导致“想得太多”，从而学习效果反而下降了。

传统但非常成功的基于n-grams的方法是通过连接训练集中非常短的重叠序列来获得泛化。

简要介绍n-grams方法：![1668773747703](D:%5CTypora%5Cuser-image%5C1668773747703.png)

因此作者建议通过学习单词的分布的表示来克服维度的诅咒，该表示允许每个训练句子向模型告知语义相邻句子的指数数量。该模型同时学习：

1. 每个单词的分布表示；
2. 词序列的概率函数，用这些分布表示的形式来表示。

> 为什么这个模型具有良好的泛化性？

因为如果一个从未见过的词序列(也就是一个句子)是由与构成一个已经见过的词序列的单词相似的(即表示是相似的)单词组成的，那么这个从未见过的词序列就具有很高的概率。

举个例子：“我叼你妈的”见过，“我日你妈”没见过，现在问“‘我日你妈’后面接什么？”，因为“叼”和“日”被模型认为是相似的表达，因此，“我日你妈”被认为是一个认识的句子，也就具有了泛化性，那么后面接的那个字就是“的”了。(我是这么理解的)。

这篇论文中的方法用的是神经网络进行计算概率函数，所以模型和数据量都十分巨大，使用了两个文本语料库，发现该方法显著改进了最先进的n-gram模型，并且所提出的方法允许利用较长的上下文。

# 一、介绍

对于离散空间，泛化结构不那么明显:这些离散变量的任何变化都可能对被估计函数的值产生剧烈的影响，当每个离散变量可以取的值的数量很大时，大多数观测对象在汉明距离上彼此之间的距离几乎是最大的。

语言的统计模型可以用给定所有前一个词的条件概率来表示：
$$
\hat{P}(w^T_1)
=\displaystyle\prod^T_{t=1}\hat{P}(w_t|w^{t-1}_1)
$$
其中，$w_t$是第t个词(单词或词语)，$w_i^j=(w_i,w_{i+1},\dots,w_{j-1},w_j)$是之前已经写下词所组成的词序列。

这种统计语言模型已经被发现在许多涉及自然语言的技术应用中很有用，例如语音识别、语言翻译和信息检索。因此，统计语言模型的改进可能对此类应用程序产生重大影响。

在建立自然语言的统计模型时，可以利用词序，以及词序中时间较近的单词在统计上更具依赖性这一事实，大大降低了建模问题的难度。因此，n-gram模型构建了下一个单词的条件概率表，对于大量上下文中的每一个，换句话说，后$n-1$个单词的组合：
$$
\hat{P}(w_t|w_1^{t-1})
\approx \hat{P}(w_t|w_{t-n+1}^{t-1})
$$
我们只考虑在训练语料库中实际出现或出现足够频繁的连续单词的组合。当一个由n个单词组成的新组合出现在训练语料库中时，会发生什么?我们不希望将这种情况的概率赋为零，因为这样的新组合很可能会发生，而且对于更大的上下文大小，它们会发生得更频繁。

一个简单的答案是查看使用较小上下文大小的预测概率，就像在反向三三元模型或光滑(或插值)三三元模型中所做的那样。那么，在这样的模型中，如何从训练语料库中看到的单词序列得到泛化到新的单词序列呢?理解这是如何发生的一种方法是考虑一个与这些内插或回退的n-gram模型对应的生成模型。

**从本质上讲，一个新的单词序列是通过“粘合”非常短和重叠的长度为1、2……或者最多n个在训练数据中经常出现的单词。**

获取下一词的概率的规则隐含在后退或内插n-gram算法的细节中。研究人员通常使用$n=3$，即三元图，并获得最先进的结果，但参见Goodman(2001)如何结合许多技巧可以产生实质性的改善。显然，在单词前面的序列中，要预测的信息要多得多，而不仅仅是前两个单词的身份。

这种方法至少有两个特点需要改进：

1. 首先，没考虑超过1或2个单词的上下文；
2. 其次，它没有考虑单词之间的“相似性”。

例如，在训练语料库中看过“The cat is walking in the bedroom”这句话，应该有助于我们归纳出“A dog was running in a room”这句话几乎相同的可能性，仅仅因为“dog”和“cat”、“the”和“a”等等，它们具有类似的语义和语法作用。

本文的一个重要贡献是表明训练这样的大尺度模型是昂贵的，但可行的，适用于大背景，并产生良好的比较结果。

## 1、用分布式表示对抗维度诅咒

> 简而言之，所建议的方法的思想可概括如下:

1. 为词汇表中的每个单词关联一个分布式词特征向量($R^m$中的实值向量)；
2. 将词序列的联合概率函数用序列中这些词的特征向量表示；
3. 同步学习词特征向量和概率函数的参数。

特征向量表示单词的不同方面:每个单词都与向量空间中的一个点相关联。特征的数量(例如在实验中m =30、60或100)远远小于词汇表的大小(例如17000)。概率函数表示为给定前一个单词的下一个单词的条件概率的乘积(例如，在实验中，使用多层神经网络在给定前一个单词的情况下预测下一个单词)。该函数具有可以迭代调优的参数，以最大化训练数据的对数似然性或正则化标准，例如通过添加权重衰减惩罚。虽然每个单词相关的特征向量是学习的，但它们可以使用语义特征的先验知识进行初始化。

在所提出的模型中，由于“相似”的词应该具有相似的特征向量，而且由于概率函数是这些特征值的平滑函数，因此特征的微小变化将导致概率的微小变化。因此，在训练数据中只出现上述句子中的一个，不仅会增加该句子的概率，还会增加其在句子空间中“邻居”的组合数量(由特征向量序列表示)。如下图：

![1668905132366](D:%5CTypora%5Cuser-image%5C1668905132366.png)

## 2、与之前工作的联系

使用神经网络建模高维离散分布的想法已经被发现对学习$Z_1,Z_2,\dots,Z_n$的联合概率很有用，$Z_1,Z_2,\dots,Z_n$是一组随机变量，其中每个变量可能具有不同的性质。在该模型中，联合概率被分解为条件概率的乘积：

![1668905680488](D:%5CTypora%5Cuser-image%5C1668905680488.png)

其中，$g(.)$是一个具有特殊从左到右结构的神经网络来表示的函数。

发现单词之间的一些相似点，从而从训练序列到新序列进行泛化的想法并不新鲜。在这里提出的模型中，没有使用离散的随机或确定性变量(对应于单词集的软或硬分区)来描述相似度，而是对每个单词使用一个连续的实向量，即一个习得的分布式特征向量，用来表示单词之间的相似度。

# 二、神经模型

训练集是一个由词$w_t\in V$构成的词序列$w_1,\dots,w_T$，其中词汇表$V$是一个庞大而有限的集合。目的是学习一个好的模型$f(w_t,\dots,w_{t-n+1})=\hat{P}(w_t|w_1^{t-1})$，从某种意义来说，它给出了很高的样本外的可能性。下面，报告了$\frac{1}{\hat{P}(w_t|w_1^{t-1})}$的几何平均，也被称为复杂度，这也是平均负对数似然的指数。模型唯一的约束条件是$对于任意的w_1^{t-1},有\displaystyle\sum_{i=1}^{|V|}f(i,w_{t-1},\dots,w_{t-n+1})=1，且f>0$。通过这些条件概率的乘积，我们得到了单词序列的联合概率模型。

将函数$f(w_t,\dots,w_{t-n+1})=\hat{P}(w_t|w_1^{t-1})$分成两部分：

1. C是从V中的任何元素i到实值向量$C(i)\in R^m$的映射。它表示与词汇表中每个单词相关的分布式特征向量。在实践中，C用$|V|\times m$的自由参数矩阵表示。
2. 词的概率函数，用C表示：函数g将输入的上下文单词的特征向量序列$(C(w_{t-n+1}),\dots,C(w_{t-1}))$映射到V中下一个单词$w_t$的条件概率分布上。g的输出是一个向量，其第i个元素估计的概率为$\hat{P}(w_t=i|w_1^{t-1})$，如图1所示。

![1668911455717](D:%5CTypora%5Cuser-image%5C1668911455717.png)

$$
f(i,w_{t-1},\dots,w_{t-n+1})=g(i,C(w_{t-1}),\dots,C(w_{t-n+1}))
$$
其中，函数f是这两个映射(C和g)的组合，C在上下文中的所有单词之间共享。这两部分的每一部分都有一些相关的参数。映射C的参数就是特征向量本身，函数g可以由前馈或递归神经网络或另一个参数化函数实现，参数为$ \omega $。全体参数集为$\theta=(C,\omega)$。

训练通过寻找θ来实现，θ使训练语料库受惩罚的对数似然值最大化：
$$
L=\frac{1}{T}\displaystyle\sum_tlogh(w_t,w_{t-1},\dots,w_{t-n+1};\theta)+R(\theta)
$$
其中$R(\theta)$是正则项。例如，在实验中，R是一个权重衰减惩罚，只应用于神经网络的权重和C矩阵，并不是偏差。

在上面的模型中，自由参数的数量只与词汇表中的单词数量V成线性比例。它也仅以n阶线性扩展:如果引入更多的共享结构，例如使用时滞神经网络或循环神经网络(或两者的组合)，则比例因子可以降低为次线性。

在下面的大多数实验中，神经网络在单词特征映射之外有一个隐藏层，并且可以选择从单词特征到输出的直接连接。因此实际上有两个隐藏层:一个是共享词特征层C，它没有非线性(它不会添加任何有用的东西)，另一个是普通的双曲正切隐藏层。更精确地说，神经网络计算以下函数，其输出层为softmax层，保证正概率和为1：
$$
\hat{P}(w_t|w_{t-1},\dots,w_{t-n+1})
=\frac{e^{y_{w_t}}}{\sum_ie^{y_i}}
$$
$y_i$是每个输出单词的非归一化对数概率，公式如下：
$$
\tag{1}
y
=b+Wx+Utanh(d+Hx)
$$
其中，双曲切线tanh对每个元素使用，W可选为0(无直接连接)，x为单词特征层激活向量，它是来自矩阵C的输入单词特征的拼接：
$$
x
=(C(w_{t-1}),C(w_{t-2}),\dots,C(w_{t-n+1}))
$$
设h为隐藏单元数，m为与每个单词相关的特征数。当不需要从单词特征到输出的直接连接时，矩阵W设为0。模型的自由参数为输出偏差b(带|V |元素)，隐含层偏差d(带h元素)，隐含到输出权重U (|V | × h矩阵)，单词特征到输出权重W (|V | × (n−1)m矩阵)，隐含层权重h (h × (n−1)m矩阵)，单词特征C (|V | × m矩阵)：
$$
\theta
=(b,d,W,U,H,C)
$$
自由参数为$|V|(1+nm+h)+h(1+(n-1)m)$。主导因子是$|V|(nm+h)$。

神经网络上的随机梯度上升是在呈现训练语料库的第t个单词后进行如下迭代更新：

![1669453755663](D:%5CTypora%5Cuser-image%5C1669453755663.png)

其中，$ \varepsilon $是学习率。注意，在每个示例之后，大部分参数都不需要更新或访问：输入窗口中没有出现的全部单词j的词特征$C(j)$。

**混合模型**。实验中发现，通过将神经网络的概率预测与插值三字母组合模型的概率预测相结合，性能得到了改善，该模型可以使用0.5的简单固定权重、学习权重(验证集上的最大似然)或一组以上下文频率为条件的权重(使用插值三字母组合中的三字母组合、双字母组合和一元模型的相同程序，这是一种混合物)。

# 三、并行实现

尽管参数的数量可以很好地扩展，即随输入窗口的大小和词汇表的大小线性增长，但获得输出概率所需的计算量要比从n-gram模型中获得的计算量大得多。

主要原因是在n-gram模型中，得到一个特定的$P(w_t|w_{t-1},\dots,w_{t-n+1})$不需要计算词汇表中所有单词的概率，因为相对频率的线性组合很容易归一化(在训练模型时执行)。神经实现的主要计算瓶颈是输出层激活的计算。

在并行计算机上运行模型(包括训练和测试)是减少计算时间的一种方法。我们探索了两种平台上的并行化:共享内存处理器机器和具有快速网络的Linux集群。

## 1、数据并行处理

## 2、参数并行处理

![1669542663718](D:%5CTypora%5Cuser-image%5C1669542663718.png)

![1669542689088](D:%5CTypora%5Cuser-image%5C1669542689088.png)

在上面的实现中没有显示权值衰减正则化，但是可以很容易地加入(通过在每次更新时从每个参数中减去权值衰减因子乘以学习率乘以参数值)。注意，参数更新是直接完成的，而不是通过参数梯度向量，以提高速度，在本实验中，计算速度的一个限制因素是对内存的访问。

在网络速度较慢的集群上，通过每K个示例(一个小批处理)执行一次通信，而不是对每个示例执行一次通信，仍然有可能获得高效的并行化。这需要在每个处理器中存储K个版本的神经网络活动和梯度。在K个例子的正向阶段之后，`概率和`必须在处理器之间共享。然后开始K个逆向阶段，得到K个部分梯度向量$\frac{∂L}{∂a}$和$\frac{∂L}{∂x}$。在处理器之间交换这些梯度向量后，每个处理器就可以完成后向阶段和更新参数，这种方法主要是节省时间，因为节省了网络通信延迟(传输的数据量相同)。如果K太大，可能会在收敛时间上损失，同理，批量梯度下降一般比随机梯度下降慢得多。

# 四、实验结果

对布朗语料库进行了比较实验，该语料库来自各种各样的英语文本和书籍，共有1181041个单词。前80万个单词用于训练，随后20万个单词用于验证(模型选择、重量衰减、早期停止)，剩下的181041个单词用于测试。不同的单词数量为47,578个(包括标点符号，区分大小写，以及用于分隔文本和段落的句法标记)。频次≤3的罕见词合并为单个符号，词汇量减少到|V | = 16,383。

为了训练神经网络，初始学习率设置为$ \varepsilon _0=10^{-3}$(在使用少量数据集进行了几次试验后)，并按照以下时间表逐步降低：$\varepsilon _1=\frac{\varepsilon _0}{1+rt}$，其中$t$表示完成的参数更新次数，$r$是递减因子，启发式选择$r=10^{-8}$。

## 1、n-gram模型

 神经网络比较的第一个基准是插值或平滑的三元模型。设$q_t=l(freq(w_{t-1},w_{t-2}))$表示为输入上下文$(w_{t-1},w_{t-2})$出现的离散化频率。那么条件概率估计具有条件混合的形式：
$$
\hat{P}(w_t|w_{t-1},w_{t-2})
=\alpha_0(q_t)p_0+\alpha_1(q_t)p_1(w_t)+\alpha_2(q_t)p_2(w_t|w_{t-1})+\alpha_3(q_t)p_3(w_t|w_{t-1},w_{t-2})
$$
其中，条件权重$\alpha_i(q_t) \geqslant 0$，$\sum_i\alpha_i(q_t)=1$。基本预测器如下：$p_0=\frac{1}{|V|}$，$p_1(i)$是一个字母组合(训练集中单词i的相对频率)，$p_2(i|j)$为二元组合(前一个单词为j时，单词i的相对频率)，$p_3(i|j,k)$为三元组合(当前两个单词是j和k时，I的相对频率)。这是因为当$(w_{t-1},w_{t-2})$的频率较大时，$p_3$是最可靠的；而当$(w_{t-1},w_{t-2})$的频率较小时，$p_2,p_1甚至p_0$的低阶统计量更可靠。$q_t$的每个离散值(是上下文频率箱)都有不同的混合权值α集。在一组数据(验证集)上，用EM算法大约5次迭代就可以很容易地估计出它们，而不用于估计一组、二组和三组相对频率。插值的n-gram被用来与mlp形成混合物，因为它们似乎以非常不同的方式犯“错误”。

## 2、结果

表明至少在较小的语料库上，在没有直接输入到输出连接的情况下，以更长的训练为代价可以获得更好的泛化:没有直接连接的网络需要两倍的时间收敛(20代而不是10代)，尽管困惑度略低。

![1669618514757](D:%5CTypora%5Cuser-image%5C1669618514757.png)

# 五、结论

在两个语料库上的实验表明，一个有超过100万个例子，另一个有超过1500万字的更大的语料库，提出的方法产生的困惑比最先进的方法平滑三字母组合(smoothed triram)要好得多，困惑度的差异在10%到20%之间。

这些改进的主要原因是，所提出的方法允许利用习得的分布式表示，用它自己的武器来对抗维数的诅咒:每个训练句子通知模型关于其他句子的组合数量。

利用时间结构并扩展输入窗口的大小以包含可能的一整段(而不增加太多的参数数量或计算时间)的一个简单想法是使用时滞和可能的循环神经网络。

更普遍地说，这里提出的工作为改进统计语言模型打开了大门，这种改进是由基于分布式表示的更紧凑、更流畅的表示取代“条件概率表”带来的，分布式表示可以容纳更多条件变量。尽管在统计语言模型(例如随机语法)中花费了大量精力来限制或总结条件变量，以避免过拟合，但这里描述的模型类型将困难转移到其他地方:需要更多的计算，但计算和内存需求是线性增长的，而不是随条件变量的数量呈指数增长。