# 摘要

关键字(keyword)或关键词(keyphrase)提取是为了识别显示文档主要主题的单词或短语。本文提出了一种混合注意模型AttentionRank，用于以无监督的方式识别文档中的关键字。AttentionRank使用`预先训练好的语言模型计算自注意力和交叉注意力`。

自注意力的目的是确定一个候选词在句子上下文中的重要性。交叉注意力是为了识别候选词与文档中句子之间的语义相关性。

使用了七个基线方法和三个数据集用来评估此方法。

结果表明，AttentionRank在长文档和短文档中都是一种有效且鲁棒的无监督关键词提取模型。

源代码在Github上。

# 一、介绍

在本研究中，提出了一种基于注意力的无监督模型——AttentionRank用于关键词提取。AttentionRank是由BERT模型的自注意力机制和分层注意检索(HAR)机制驱动的。

AttentionRank模型通过计算候选词自注意力和交叉注意力来对其重要性(也就是得分)进行排序。自注意力强调候选词在句子中的重要性，交叉注意力识别候选词和文档之间的语义相关性。

交叉注意力计算候选词的embedding和文档中句子embedding之间的词级双向注意力，然后为候选词生成增强的文档embedding。

候选词的最终排名由自注意力和交叉注意力共同决定。基于文档频率的后处理步骤用于删除特定语料库的通用术语。

AttentionRank可以推广到任何领域的文档。本研究主要探讨预先训练的非领域特定语言模型能否用于关键词提取。

> 本文主要贡献：

- 提出了一种基于注意力的无监督关键词提取模型——AttentionRank；
- 证明了预训练的语言模型可以通过自注意力和交叉注意力来识别关键字；
- 实验表明，AttentionRank模型优于基线方法，并且在不同领域的短文档和长文档中识别关键短语方面是稳健的。

# 二、方法论

模型的总体架构如图1所示。AttentionRank将累积的自注意力分量和交叉注意力分量结合起来，计算出候选词的最终分数。

![1671189776829](D:%5CTypora%5Cuser-image%5C1671189776829.png)

> 提出的模型有四个主要步骤：

1. 从文档生成候选集C；
2. 对于每一个候选词c，$c\in C$，计算它的累加自注意力值$a_c$；
3. 计算候选词c和文档d之间的交叉注意力相关性$(r_c)$；
4. 通过$a_c$和$r_c$的线性组合计算每个候选人的最终得分$s_c$。

## 1、候选词生成

该模型使用了EmbedRank中的候选词提取模块。本模块首先使用词性(PoS)来识别标记为NN, NNS, NNP, NNPS, JJ等的单词。然后，使用python包NLTK生成作为候选词的名词短语。

给定一句话“Most parameters of the program are controllable by experimenter-edited text files or simple switches in the program code, thereby minimizing the need for programming to create new experiments”，提取的候选词是“program code”，“experimenter-edited text files”，“need”，“parameters”，“simple switches”，“program”，和“new experiments”。

## 2、累加自注意力计算

从预训练的BERT中提取单词的自注意力权重。将一个词(w)在同一个句子(s)中从其他词($w'$)得到的注意力($a_{w'w}$)相加，相加之和为该词在一个句子(s)中的注意力值($a_w$)，如公式1所示。这个注意力值($a_w$)表示单词在句子上下文中的重要性。
$$
\tag{1}
a_w=\displaystyle\sum_{w'\in s}a_{w'w}
$$
如图2所示，所有突出显示的都是名词块。直观地说，名词块颜色越深，它得到的自注意力就越高，就有更高的概率被选为关键字。

![1671189868003](D:%5CTypora%5Cuser-image%5C1671189868003.png)

为了计算句子i中候选词(c)的自注意力，将c中单词的注意力加起来，如式2所示：
$$
\tag{2}
a_i^c=\displaystyle\sum_{w\in c}a_w
$$
候选词c的文档级自注意力为文档d每句中c的所有自注意值之和：
$$
\tag{3}
a_c=\displaystyle\sum_{i\in d}a_i^c
$$

## 3、交叉注意力计算

交叉注意模型是受`层次注意检索模型`和`双向注意模型`的启发而提出的。基于它们的架构，开发出了交叉注意组件来衡量基于上下文的候选词和文档之间的相关性。

预训练的BERT模型可以生成候选词c(c表示的是短语，c由多个w组成，w表示一个单词)表示，如$E^c=\{e^c_1,\dots,e^c_m\}$，其中$e_i\in R^H$是$w_i$的embedding，候选对象有m个单词。类似地，预训练的BERT模型也可以生成一个表示$E^i=\{e^i_1,\dots,e^i_n \}$表示包含n个单词的句子i。

交叉注意计算一个新的文档embedding，以更好地衡量候选词和文档中句子之间的上下文相关性。给定句子i表示为$E^i\in R^{n\times H}$，候选词c表示为$E^c\in R^{m\times H}$，则i与c之间的相似性矩阵$S\in R^{n\times m}$如式4所示：

![1671241638692](D:%5CTypora%5Cuser-image%5C1671241638692.png)

那么，基于词的句子到候选词的相似度以及候选词到句子的相似度可测量如式5和式6：

![1671241706214](D:%5CTypora%5Cuser-image%5C1671241706214.png)

一行是一个句子，一列是一个候选词(也能理解成候选短语，反正已经被弄成一块了)。

基于词的交叉注意权重从句子到候选词和从候选词到句子计算如式7和式8所示：

![1671242024927](D:%5CTypora%5Cuser-image%5C1671242024927.png)

新的句子$V^i$建立在这些交叉注意权重之上，并通过计算四项的总和的平均值来计算，如公式9所示：

![1671243901925](D:%5CTypora%5Cuser-image%5C1671243901925.png)

其中，$E^i$是句子的原始语境，$A_{i2c}$，$E^i \odot A_{i2c}$和$E^i \odot A_{c2i}$衡量了句子和候选词之间的上下文相关性，$\odot$是逐元素乘法。

新的句子$V^i$仍然是一组embedding，由候选词和句子之间基于词的关系组成。为了生成基于$V^i$的标准化句子表示，在应用交叉注意后，对$V^i$进行自注意力，以突出词语的重要性。给出一个新的句子$V^i=\{v^i_1,\dots,v^i_n \}$由n个单词组成，句子i的自注意力计算如式10所示：

![1671245661196](D:%5CTypora%5Cuser-image%5C1671245661196.png)

然后，计算列平均值以获得句子$\alpha^i\in R^H$的最终表示。

![1671245706996](D:%5CTypora%5Cuser-image%5C1671245706996.png)

生成句子embeding后，我们对句子embeding执行类似的过程来生成文档embeding。给定一个文档d，其中包含一组句子$E^d=\{\alpha^1,\dots,\alpha^i \}$，为了计算文档embedding，首先需要生成文档的自注意力来强调与候选对象相关性较高的句子(式12)，然后逐列平均得到最终的文档embedding$p^d\in R^H$。

![1671245966774](D:%5CTypora%5Cuser-image%5C1671245966774.png)

由于候选词最初也表示为单词embedding集$E^c=\{e^c_1,\dots,e^c_m\}$，在进行自注意力计算(式14)，然后逐列平均，得到最终候选词$p^c\in R^H$，如式15：

![1671246151428](D:%5CTypora%5Cuser-image%5C1671246151428.png)

最后，候选词c和文档d之间的相关性由$p^c$和$p^d$的余弦相似度决定，如式16所示：

![1671247981029](D:%5CTypora%5Cuser-image%5C1671247981029.png)

## 4、最终得分的计算和后期处理

对于文档d，分别计算每个候选词的累加注意力$a_c$和交叉注意力$r_c$并做归一化处理。候选词的最终分数是用公式17对这两个值进行线性积分，其中d∈[0,1]。

![1671261880455](D:%5CTypora%5Cuser-image%5C1671261880455.png)

语料库通常是特定于领域的。这意味着一些高文档频率的单词可能是这个语料库的通用单词。在本研究中，为了限制通用词或短语成为关键短语，删除了文档频率高于阈值$df_{\theta}$的候选词。

# 三、实验

## 1、数据集和评估指标

表1提供了每个文档的n-grams百分比、平均字数(AveWords)和平均句子数(AveSentences)。

![1671263383087](D:%5CTypora%5Cuser-image%5C1671263383087.png)

## 2、基线方法

SingleRank、RAKE、TopicRank、PositionRank、YAKE!、EmbedRank、SIFRank，一共七个基线方法。

## 3、超参数设置和计算成本

针对不同语料库，对文档频率阈值$df_{\theta}$和积分比d进行微调，以达到最佳性能。数据集Inspec、SemEval2017和SemEval2010的文档频率阈值$d f_θ$分别设置为5、5和44。对于所有数据集，设置线性组合比d为0.8。对于基线方法，使用了发布在相应GitHub上的参数。

## 4、结果

表2显示了在所有数据集上使用AttentionRank和基线模型的召回率、精度和F1 @5、10和15(这些数字表示是用前多少名的候选词)的结果。

![1671264326953](D:%5CTypora%5Cuser-image%5C1671264326953.png)

表3为参考文献报道与其他无监督关键词提取方法的对比结果。

![1671264604302](D:%5CTypora%5Cuser-image%5C1671264604302.png)

实验表明，基于embedding的算法，包括AttentionRank算法，在短文档上的表现优于基于统计的算法和基于图的算法。

由于累积自注意力模型考虑了候选词对文档的自注意力累积，因此AttentionRank在长文档集上工作得更好。

PR曲线(如图3所示)使用每种方法生成的排名前60的候选词进行总体比较。

![1671265727617](D:%5CTypora%5Cuser-image%5C1671265727617.png)

## 5、消融研究(控制变量实验)

### 5.1、集成比例

从图4可以看出，总体上，累积自我注意值的贡献大于交叉注意相关性。

![1671265991805](D:%5CTypora%5Cuser-image%5C1671265991805.png)

但是，每个数据集的最佳比例是不同的。例如，对于长文档集SemEval2010，当d = 1时性能得到优化，这意味着只需要累积的自我注意值就可以找到关键短语；对于短文档集，如Inspec，交叉注意相关性的影响更大。

### 5.2、文档频次分析

图5显示，不同的数据集可以根据$d f_θ$值有不同的最佳性能。

![1671267354583](D:%5CTypora%5Cuser-image%5C1671267354583.png)

对于较短的文档数据集，最佳$d f_θ$通常较小。长文档数据集的$d f_θ$值相对较大。当$d f_θ$大于某一值后，性能随着$d f_θ$的增加而下降，这意味着$d f_θ$较大的项可能是特定语料库中文档的关键短语。

## 6、案例研究

# 四、相关工作

就是扯扯古往今来的东西。

# 五、结论

研究了积累自注意力机制与交叉注意力模型相结合的无监督关键词提取方法。`利用预训练BERT模型计算自注意值和交叉注意值。`候选词通过自注意力和交叉注意力相关性来获得最终的排名分数。

我们将提出的AttentionRank模型与三个基准数据集(包括两个短文档数据集和一个长文档数据集)上的七个不同基线进行了比较。AttentionRank在所有数据集上获得了更好或有竞争力的F1@5, 10和15。

消融研究表明，累积的自注意力对长文档集的相关性评分的贡献高于交叉注意力。对于较短的文档集，两种注意机制的线性整合表现出较好的性能。