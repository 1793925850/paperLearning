# 摘要

随着可用文本数据数量的增加，开发能够自动分析、分类和总结这些数据的算法已经成为一种必要。

在这项研究中，作者提出了一种新的关键字识别算法，即提取代表给定文档关键方面的一个或多个单词短语，称为基于Transformer的关键字识别神经标记器(TNT-KID)。通过为手头的特定任务调整Transformer架构，并利用特定领域语料库上的语言模型预训练，该模型能够克服有监督和无监督的最先进的关键字提取方法的缺陷，在各种不同的数据集上具有竞争力和强大的性能，同时只需要性能最好的系统所需的一小部分手动标记数据。

本研究还提供了全面的误差分析，并对模型的内部工作原理进行了有价值的见解，并对关键字识别工作流的特定组件对整体性能的影响进行了消融研究。

# 一、介绍

本算法，只需要其他神经方法所需的手动标记数据的一小部分，但和这些最先进的监督方法的性能一样。

利用了迁移学习。

其中关键字标注器首先以无监督的方式作为语言模型在大型语料库上进行训练，然后在(通常)使用手动标记关键字的小型语料库上进行微调。通过对计算机科学文章和新闻两个不同领域的实验，表明语言模型预训练使算法能够成功地适应特定领域并掌握文本的语义信息，从而大大减少了训练关键字检测器所需的标记数据量。

BERT牛逼。BERT是英语的模型。

作者将关键字提取任务制定为序列标记任务。

# 二、相关工作

本节概述了关键字提取的选择方法。

## 1、监督关键词提取方法

第一个有监督关键词提取算法：KEA。仅使用TfIdf和术语在文本中的位置作为术语识别的特征。这些特征被馈送到朴素贝叶斯分类器，该分类器用于确定文本中的每个单词或短语是否为关键字。

## 2、无监督关键词提取方法

总的来说，无监督的关键词提取方法分为四类：统计、增强图、embedding、语言模型。

# 三、方法论

## 1、架构

该模型遵循原始Transformer编码器的架构设计，如图1(a)所示。编码器由规范化层组成，然后是多头注意机制。在注意机制周围使用残留连接，然后进行另一层规范化。接下来是完全连接的前馈和dropout层，在其周围使用另一个残余连接。![image-20230711113642565](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202307111136636.png)

对于两个不同的训练阶段，**语言模型预训练**和**微调**，两个不同的“头”被添加到编码器的顶部，这两个阶段是相同的，因此允许权重从预训练阶段转移到微调阶段。语言模型头预测词汇表中每个单词出现在序列中特定位置的概率，并由dropout层和前馈层组成，前馈层返回大小为SL * |V|的输出矩阵，其中SL表示序列长度(即输入文本中的单词数)，|V|表示词汇表大小。接下来是自适应softmax层。

将关键字提取建模为**二元分类任务**。

提出了注意机制的重新参数化(见图1(b))。

![image-20230711114332969](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202307111143018.png)

还试验了额外的词性(POS)标记序列作为输入。首先向量化该序列，然后将其添加到词嵌入矩阵中。

第三种修改涉及**用自适应输入表示和自适应softmax**替换**标准输入嵌入层和softmax函数**。这大大降低了模型的内存需求和时间复杂度，但代价是性能略有下降。

实验了两种标记化方案：

1. 单词标记化
2. 句子片段字节对编码

## 2、迁移学习

考虑两个不同的预训练目标：

1. 自回归语言模型
2. 隐(masked)语模型

![image-20230712102832426](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202307121028575.png)

## 3、关键词识别

如图2所示。

注意，1的序列总是被解释为一个多词关键字，而不是一个单词关键字的组合。

过滤：

1. 如果一个关键词超过四个字，它将被丢弃。
2. 包含标点符号(破折号和撇号除外)的关键字将被删除。
3. 根据模型分配的softmax概率，将检测到的关键字按降序排列。

# 四、实验

## 1、数据集

计算科学领域：

- KP20k
- Inspec
- Krapivin
- NUS
- SemEval

新闻领域：

- KPTimes
- JPTimes
- DUC

## 2、实验设计

1. 首先，对所有数据集进行小写和标记。在这两种标记化方案中，使用一个特殊的< eos >标记来表示每个句子的结尾。
2. 预训练。
3. 在预训练阶段之后，训练好的语言模型在每个数据集的验证集上进行微调，其中80%的文档用于微调，20%的文档用于超参数优化和测试集模型选择。
4. 包含超过512个token的文档将被截断。根据令牌长度对文档进行排序，并分成几批。每个批处理中的文档都用一个特殊的< pad >标记填充到批处理中最长文档的长度。

## 3、评估

F1、P、R。(经典三剑客)

## 4、关键词提取结果和比较的艺术状态

# 5、误差分析

## 1、TNT -KID与CatSeqD的比较

## 2、CatSeqD微调

大多数监督方法的主要缺点是它们需要一个带有标记关键字的大型数据集来进行训练，这至少在某些语言中是稀缺的。

## 3、剖析注意力空间

transformer架构的优点之一是它采用了注意力机制，可以对其进行分析和可视化，为系统的内部工作提供了有价值的见解，并能够解释神经网络如何处理关键字识别任务。

# 六、消融实验

