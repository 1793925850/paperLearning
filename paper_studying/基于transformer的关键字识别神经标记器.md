# 摘要

随着可用文本数据数量的增加，开发能够自动分析、分类和总结这些数据的算法已经成为一种必要。

在这项研究中，作者提出了一种新的关键字识别算法，即提取代表给定文档关键方面的一个或多个单词短语，称为基于Transformer的关键字识别神经标记器(TNT-KID)。通过为手头的特定任务调整Transformer架构，并利用特定领域语料库上的语言模型预训练，该模型能够克服有监督和无监督的最先进的关键字提取方法的缺陷，在各种不同的数据集上具有竞争力和强大的性能，同时只需要性能最好的系统所需的一小部分手动标记数据。

本研究还提供了全面的误差分析，并对模型的内部工作原理进行了有价值的见解，并对关键字识别工作流的特定组件对整体性能的影响进行了消融研究。

# 一、介绍

本算法，只需要其他神经方法所需的手动标记数据的一小部分，但和这些最先进的监督方法的性能一样。

利用了迁移学习。

其中关键字标注器首先以无监督的方式作为语言模型在大型语料库上进行训练，然后在(通常)使用手动标记关键字的小型语料库上进行微调。通过对计算机科学文章和新闻两个不同领域的实验，表明语言模型预训练使算法能够成功地适应特定领域并掌握文本的语义信息，从而大大减少了训练关键字检测器所需的标记数据量。

BERT牛逼。BERT是英语的模型。

作者将关键字提取任务制定为序列标记任务。

# 二、相关工作

本节概述了关键字提取的选择方法。

## 1、监督关键词提取方法

第一个有监督关键词提取算法：KEA。仅使用TfIdf和术语在文本中的位置作为术语识别的特征。这些特征被馈送到朴素贝叶斯分类器，该分类器用于确定文本中的每个单词或短语是否为关键字。

## 2、无监督关键词提取方法

总的来说，无监督的关键词提取方法分为四类：统计、增强图、embedding、语言模型。

# 三、方法论

## 1、架构

该模型遵循原始Transformer编码器的架构设计，如图1(a)所示。编码器由规范化层组成，然后是多头注意机制。在注意机制周围使用残留连接，然后进行另一层规范化。接下来是完全连接的前馈和dropout层，在其周围使用另一个残余连接。![image-20230711113642565](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202307111136636.png)

对于两个不同的训练阶段，**语言模型预训练**和**微调**，两个不同的“头”被添加到编码器的顶部，这两个阶段是相同的，因此允许权重从预训练阶段转移到微调阶段。语言模型头预测词汇表中每个单词出现在序列中特定位置的概率，并由dropout层和前馈层组成，前馈层返回大小为SL * |V|的输出矩阵，其中SL表示序列长度(即输入文本中的单词数)，|V|表示词汇表大小。接下来是自适应softmax层。

将关键字提取建模为**二元分类任务**。

提出了注意机制的重新参数化(见图1(b))。

![image-20230711114332969](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202307111143018.png)

还试验了额外的词性(POS)标记序列作为输入。首先向量化该序列，然后将其添加到词嵌入矩阵中。

第三种修改涉及**用自适应输入表示和自适应softmax**替换**标准输入嵌入层和softmax函数**。这大大降低了模型的内存需求和时间复杂度，但代价是性能略有下降。

实验了两种标记化方案：

1. 单词标记化
2. 句子片段字节对编码

## 2、转移学习
